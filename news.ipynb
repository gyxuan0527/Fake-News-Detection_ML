{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "52bf27bebf8e95c02bb8b123a1d535075e8376c4"
   },
   "source": [
    "# Fake News Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "414a6b7e831613c3c9a88d9633fa412de72ebd8a"
   },
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv(\"train.csv\", encoding='ISO-8859-1',delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout dirty data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "2df12f6473bd02427771d3ca0ae2de291ec03b83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text     content\n",
      "label      label\n",
      "Name: 1615, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_data.loc[1615])\n",
    "train_data = train_data.drop([1615])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "20b16d52f53f8b3005e5728ee36b3e3b8f5e6461"
   },
   "source": [
    "## Visualize the news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "c2dfe4d1b2a68571cf47b270e61d7ec01458efa6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "606     This year, Colorado produced the most Winter O...\n",
       "3040    Ted Cruz' Full 'Meet the Press' InterviewRepub...\n",
       "3911    Shay Mitchell, who frequently documents her va...\n",
       "1367    Cheryl Burke from Dance Moms came out in suppo...\n",
       "4399    Jaime Pressly is opening up about how expectin...\n",
       "2269    One of the most contentious issues in the Stat...\n",
       "4684    45th President of the United States  Donald Jo...\n",
       "4896    Who is Prince Harry's father? That's what many...\n",
       "2475    Erika Jayne has been keeping a secret.  Get pu...\n",
       "2763    (CNN) The Tony Awards celebrated the best of B...\n",
       "635     (Excerpt) Read more at: E! Online  Wake Up To ...\n",
       "2171    In light of the Timeâs Up movement, Bella Th...\n",
       "4621    Itâs complicated for Teen Mom 2âs Kailyn L...\n",
       "3616    Selena Gomez & Marshmello Deliver Intense 'Wol...\n",
       "693     When members of the royal family tie the knot,...\n",
       "3408    Asa Soltan Rahmati welcomed her first child in...\n",
       "477     Inside BeyoncÃ©'s Weight Loss Journey 4 Months...\n",
       "3415    Juggling successful careers in both music and ...\n",
       "667     Best Director: âManchester by the Seaâ  â...\n",
       "4911    Not okay.  Sexism is so rife in Hollywood it h...\n",
       "4040    Kate Noelle \"Katie\" Holmes (born December 18, ...\n",
       "1855    If youâre looking for a way to cool down thi...\n",
       "1427    Bethenny Frankel is all about the hustle. Gary...\n",
       "2941    â Speaking of power, let me ask you a questi...\n",
       "1884    Good news â Josh Charles is going to be a da...\n",
       "4852    Angelina Jolie has officially moved on from he...\n",
       "4530    Kylie Jenner was spotted out and about with tw...\n",
       "1888    CLOSE Nickelodeon and Paramount Pictures have ...\n",
       "3211    Since about 2015, America has become increasin...\n",
       "2269    One of the most contentious issues in the Stat...\n",
       "2474    Game of Thrones has a lot of great storylines ...\n",
       "4501    Welcome to the top 32!  Alpha Male Madness 201...\n",
       "2035    Fast & Furious is going global.  Vin Diesel to...\n",
       "4244    Blac Chyna is defending herself as a caring mo...\n",
       "736     Muslim convert and would-be domestic terrorist...\n",
       "3185    These two! Keith Urban posted an adorable mess...\n",
       "117     Image copyright EPA Image caption Rebel Wilson...\n",
       "1214    LOS ANGELESâ(Spoiler alert: This column has ...\n",
       "4699    A photo of Nicolas Cage is sweeping the intern...\n",
       "1124    UPDATE: 2/13/2017: Billy Bush is officially on...\n",
       "4631    T.I. has a few words for Rob Kardashian after ...\n",
       "3644    Poor Mandy Teefey. If Selena Gomez's mom was h...\n",
       "3622    CLOSE Aaron Carter struggles with this medical...\n",
       "1730    Entourage and Wisdom of the Crowd star Jeremy ...\n",
       "396     Fish in a Pit Ambushes Another Fish That Comes...\n",
       "263     Independence Day director Roland Emmerich sent...\n",
       "4378    Â© Gossip Cop  Kathy Griffin released Harvey L...\n",
       "112     Because we see time moving forward in a stream...\n",
       "123     Scrunchies are so â90sâ¦which explains why ...\n",
       "4548    âIt wasnât until I began to work out in ea...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_indexs = np.random.randint(1,len(train_data),50).tolist()\n",
    "train_data[\"text\"][rand_indexs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e1ae5c3aa81d998b37d87fa926d9682cd71358f2"
   },
   "source": [
    "## Emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "406019e45c7d1c06f1739be273534a42e8002f42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{': ',\n",
       " ':(',\n",
       " ':)',\n",
       " '::',\n",
       " ':D',\n",
       " '; ',\n",
       " 'X ',\n",
       " 'X)',\n",
       " 'X,',\n",
       " 'X.',\n",
       " 'X2',\n",
       " 'X3',\n",
       " 'X:',\n",
       " 'XD',\n",
       " 'XL',\n",
       " 'XM',\n",
       " 'XO',\n",
       " 'XQ',\n",
       " 'XS',\n",
       " 'XT',\n",
       " 'XX',\n",
       " 'Xi',\n",
       " 'Xo',\n",
       " \"x'.\",\n",
       " 'x2',\n",
       " 'xo',\n",
       " 'xx'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "news = train_data.text.str.cat()\n",
    "a = r\" ([xX:;][-']?.) \"\n",
    "emos = set(re.findall(a,news))  \n",
    "\n",
    "emos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a0b0964a5bab89f2f9e235bb5a6bf384fe86c3c6"
   },
   "source": [
    "## Most used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "d3eea0c5a024a8844060fea507a5ff374d765254"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def most_used_words(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    frequency_dist = nltk.FreqDist(tokens)  \n",
    "    print(\"There is %d different words\" % len(set(tokens)))\n",
    "    \n",
    "    return sorted(frequency_dist,key=frequency_dist.__getitem__, reverse=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "44c3a1b1bf2b665c537534bddebc1b78714a8f73",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 122575 different words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " 'the',\n",
       " '.',\n",
       " 'and',\n",
       " 'to',\n",
       " 'a',\n",
       " 'of',\n",
       " 'in',\n",
       " 'that',\n",
       " 'on',\n",
       " 'for',\n",
       " 'was',\n",
       " 'her',\n",
       " 'with',\n",
       " 'is',\n",
       " '[',\n",
       " ']',\n",
       " \"'s\",\n",
       " 'I',\n",
       " 'The',\n",
       " '``',\n",
       " ':',\n",
       " 'she',\n",
       " 'it',\n",
       " \"''\",\n",
       " 'as',\n",
       " 'at',\n",
       " ')',\n",
       " '(',\n",
       " 'his',\n",
       " 'he',\n",
       " 'have',\n",
       " 'be',\n",
       " 'by',\n",
       " 'you',\n",
       " 'from',\n",
       " 'has',\n",
       " 'an',\n",
       " 'not',\n",
       " 'this',\n",
       " 'their',\n",
       " 'are',\n",
       " 'about',\n",
       " 'who',\n",
       " 'they',\n",
       " 'but',\n",
       " 'had',\n",
       " 'said',\n",
       " 'we',\n",
       " 'â\\x80\\x9d',\n",
       " '!',\n",
       " 'In',\n",
       " 'been',\n",
       " '?',\n",
       " 'out',\n",
       " 'were',\n",
       " 'up',\n",
       " 'all',\n",
       " 'one',\n",
       " 'after',\n",
       " 'will',\n",
       " 'also',\n",
       " 'which',\n",
       " 'â\\x80\\x94',\n",
       " 'when',\n",
       " 'more',\n",
       " \"'\",\n",
       " 'so',\n",
       " \"n't\",\n",
       " 'time',\n",
       " 'just',\n",
       " 'or',\n",
       " 'my',\n",
       " 'like',\n",
       " 'first',\n",
       " 'do',\n",
       " 'would',\n",
       " 'She',\n",
       " 'what',\n",
       " 'him',\n",
       " 'people',\n",
       " 'It',\n",
       " 'me',\n",
       " 'And',\n",
       " 'two',\n",
       " 'show',\n",
       " 'can',\n",
       " 'He',\n",
       " '2017',\n",
       " 'years',\n",
       " 'know',\n",
       " 'there',\n",
       " 'our',\n",
       " 'get',\n",
       " 'over',\n",
       " 'into',\n",
       " 'told',\n",
       " 'new',\n",
       " 'now',\n",
       " 'other']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_used_words(train_data.text.str.cat())[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "27922113ab86ed096339614ee6616dfd6cf259fb"
   },
   "source": [
    "## Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "760928a6057f2b8480d3c4f10ed3b2709ff62c6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 122575 different words\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "mw = most_used_words(train_data.text.str.cat())\n",
    "most_words = []\n",
    "for w in mw:\n",
    "    if len(most_words) == 1000:\n",
    "        break\n",
    "    if w in stopwords.words(\"english\"):\n",
    "        continue\n",
    "    else:\n",
    "        most_words.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "e8150fd219b8c83541820f373e345baaae29c1e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '#',\n",
       " '$',\n",
       " '&',\n",
       " \"'\",\n",
       " \"''\",\n",
       " \"'d\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'ve\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '-',\n",
       " '--',\n",
       " '.',\n",
       " '...',\n",
       " '/',\n",
       " '1',\n",
       " '10',\n",
       " '100',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '2',\n",
       " '20',\n",
       " '2003',\n",
       " '2004',\n",
       " '2005',\n",
       " '2006',\n",
       " '2007',\n",
       " '2008',\n",
       " '2009',\n",
       " '2010',\n",
       " '2011',\n",
       " '2012',\n",
       " '2013',\n",
       " '2014',\n",
       " '2015',\n",
       " '2016',\n",
       " '2017',\n",
       " '2018',\n",
       " '21',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '29',\n",
       " '3',\n",
       " '30',\n",
       " '31',\n",
       " '4',\n",
       " '40',\n",
       " '5',\n",
       " '50',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " '@',\n",
       " 'A',\n",
       " 'ABC',\n",
       " 'Academy',\n",
       " 'According',\n",
       " 'Actress',\n",
       " 'Advertisement',\n",
       " 'After',\n",
       " 'All',\n",
       " 'Although',\n",
       " 'America',\n",
       " 'American',\n",
       " 'Americans',\n",
       " 'An',\n",
       " 'And',\n",
       " 'Angeles',\n",
       " 'Angelina',\n",
       " 'Aniston',\n",
       " 'April',\n",
       " 'Are',\n",
       " 'As',\n",
       " 'At',\n",
       " 'August',\n",
       " 'Award',\n",
       " 'Awards',\n",
       " 'B',\n",
       " 'Beckham',\n",
       " 'Ben',\n",
       " 'Best',\n",
       " 'BeyoncÃ©',\n",
       " 'Bieber',\n",
       " 'Big',\n",
       " 'Billboard',\n",
       " 'Black',\n",
       " 'Blake',\n",
       " 'Brad',\n",
       " 'British',\n",
       " 'Brown',\n",
       " 'Bush',\n",
       " 'But',\n",
       " 'By',\n",
       " 'CBS',\n",
       " 'California',\n",
       " 'Chicago',\n",
       " 'Chris',\n",
       " 'Christmas',\n",
       " 'City',\n",
       " 'Clinton',\n",
       " 'Clooney',\n",
       " 'Cruise',\n",
       " 'Cyrus',\n",
       " 'Daily',\n",
       " 'David',\n",
       " 'Day',\n",
       " 'DeGeneres',\n",
       " 'December',\n",
       " 'Diana',\n",
       " 'Do',\n",
       " 'Doctor',\n",
       " 'Donald',\n",
       " 'Duchess',\n",
       " 'During',\n",
       " 'E',\n",
       " 'Elizabeth',\n",
       " 'Ellen',\n",
       " 'Emma',\n",
       " 'Entertainment',\n",
       " 'Facebook',\n",
       " 'Family',\n",
       " 'February',\n",
       " 'Film',\n",
       " 'First',\n",
       " 'For',\n",
       " 'Fox',\n",
       " 'Friday',\n",
       " 'From',\n",
       " 'George',\n",
       " 'Get',\n",
       " 'Getty',\n",
       " 'Golden',\n",
       " 'Gomez',\n",
       " 'Good',\n",
       " 'Harry',\n",
       " 'Harvey',\n",
       " 'He',\n",
       " 'Her',\n",
       " 'His',\n",
       " 'Hollywood',\n",
       " 'Holmes',\n",
       " 'House',\n",
       " 'How',\n",
       " 'However',\n",
       " 'I',\n",
       " 'If',\n",
       " 'Images',\n",
       " 'In',\n",
       " 'Instagram',\n",
       " 'Is',\n",
       " 'It',\n",
       " 'Itâ\\x80\\x99s',\n",
       " 'Iâ\\x80\\x99m',\n",
       " 'Iâ\\x80\\x99ve',\n",
       " 'Jackson',\n",
       " 'James',\n",
       " 'January',\n",
       " 'Jen',\n",
       " 'Jenner',\n",
       " 'Jennifer',\n",
       " 'Joe',\n",
       " 'John',\n",
       " 'Jolie',\n",
       " 'July',\n",
       " 'June',\n",
       " 'Just',\n",
       " 'Justin',\n",
       " 'Kanye',\n",
       " 'Kardashian',\n",
       " 'Kardashians',\n",
       " 'Kate',\n",
       " 'Kelly',\n",
       " 'Kevin',\n",
       " 'Khloe',\n",
       " 'Kim',\n",
       " 'King',\n",
       " 'Kourtney',\n",
       " 'Kris',\n",
       " 'Kylie',\n",
       " 'Last',\n",
       " 'Life',\n",
       " 'Like',\n",
       " 'Little',\n",
       " 'Live',\n",
       " 'London',\n",
       " 'Lopez',\n",
       " 'Los',\n",
       " 'Love',\n",
       " 'MTV',\n",
       " 'Man',\n",
       " 'March',\n",
       " 'Markle',\n",
       " 'Matt',\n",
       " 'May',\n",
       " 'Me',\n",
       " 'Meghan',\n",
       " 'Michael',\n",
       " 'Michelle',\n",
       " 'Miss',\n",
       " 'Monday',\n",
       " 'More',\n",
       " 'Most',\n",
       " 'Mr.',\n",
       " 'Music',\n",
       " 'My',\n",
       " 'NBC',\n",
       " 'National',\n",
       " 'Netflix',\n",
       " 'New',\n",
       " 'News',\n",
       " 'Nick',\n",
       " 'Night',\n",
       " 'No',\n",
       " 'North',\n",
       " 'Not',\n",
       " 'November',\n",
       " 'Now',\n",
       " 'OF',\n",
       " 'Obama',\n",
       " 'October',\n",
       " 'Of',\n",
       " 'On',\n",
       " 'One',\n",
       " 'PEOPLE',\n",
       " 'Palace',\n",
       " 'Paris',\n",
       " 'Paul',\n",
       " 'People',\n",
       " 'Perry',\n",
       " 'Photo',\n",
       " 'Pitt',\n",
       " 'Post',\n",
       " 'President',\n",
       " 'Prince',\n",
       " 'Princess',\n",
       " 'Queen',\n",
       " 'RELATED',\n",
       " 'Read',\n",
       " 'Real',\n",
       " 'Regina',\n",
       " 'Robert',\n",
       " 'Royal',\n",
       " 'Ryan',\n",
       " 'Sarah',\n",
       " 'Saturday',\n",
       " 'Scott',\n",
       " 'Season',\n",
       " 'See',\n",
       " 'Selena',\n",
       " 'Senator',\n",
       " 'September',\n",
       " 'Series',\n",
       " 'She',\n",
       " 'Shelton',\n",
       " 'Show',\n",
       " 'Smith',\n",
       " 'So',\n",
       " 'Star',\n",
       " 'States',\n",
       " 'Stone',\n",
       " 'Street',\n",
       " 'Sunday',\n",
       " 'Swift',\n",
       " 'THE',\n",
       " 'TMZ',\n",
       " 'TO',\n",
       " 'TV',\n",
       " 'Taylor',\n",
       " 'That',\n",
       " 'The',\n",
       " 'Their',\n",
       " 'There',\n",
       " 'They',\n",
       " 'Things',\n",
       " 'This',\n",
       " 'Thursday',\n",
       " 'Time',\n",
       " 'Times',\n",
       " 'To',\n",
       " 'Tom',\n",
       " 'Top',\n",
       " 'Trump',\n",
       " 'Tuesday',\n",
       " 'Twitter',\n",
       " 'U.S.',\n",
       " 'UK',\n",
       " 'US',\n",
       " 'USA',\n",
       " 'United',\n",
       " 'University',\n",
       " 'Up',\n",
       " 'Us',\n",
       " 'Victoria',\n",
       " 'WINNER',\n",
       " 'War',\n",
       " 'Washington',\n",
       " 'We',\n",
       " 'Wednesday',\n",
       " 'Weekly',\n",
       " 'Weinstein',\n",
       " 'Well',\n",
       " 'West',\n",
       " 'What',\n",
       " 'When',\n",
       " 'While',\n",
       " 'White',\n",
       " 'Who',\n",
       " 'Why',\n",
       " 'Will',\n",
       " 'William',\n",
       " 'Williams',\n",
       " 'With',\n",
       " 'World',\n",
       " 'Year',\n",
       " 'York',\n",
       " 'You',\n",
       " 'Your',\n",
       " '[',\n",
       " ']',\n",
       " '``',\n",
       " 'able',\n",
       " 'abuse',\n",
       " 'according',\n",
       " 'accused',\n",
       " 'across',\n",
       " 'acting',\n",
       " 'actor',\n",
       " 'actress',\n",
       " 'actually',\n",
       " 'added',\n",
       " 'adding',\n",
       " 'admitted',\n",
       " 'age',\n",
       " 'ago',\n",
       " 'album',\n",
       " 'allegations',\n",
       " 'alleged',\n",
       " 'almost',\n",
       " 'along',\n",
       " 'alongside',\n",
       " 'already',\n",
       " 'also',\n",
       " 'always',\n",
       " 'amazing',\n",
       " 'among',\n",
       " 'announced',\n",
       " 'another',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'appearance',\n",
       " 'appeared',\n",
       " 'around',\n",
       " 'artist',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'attended',\n",
       " 'attention',\n",
       " 'audience',\n",
       " 'awards',\n",
       " 'away',\n",
       " 'baby',\n",
       " 'back',\n",
       " 'bad',\n",
       " 'band',\n",
       " 'based',\n",
       " 'beautiful',\n",
       " 'became',\n",
       " 'become',\n",
       " 'began',\n",
       " 'behind',\n",
       " 'believe',\n",
       " 'best',\n",
       " 'better',\n",
       " 'big',\n",
       " 'biggest',\n",
       " 'birth',\n",
       " 'birthday',\n",
       " 'bit',\n",
       " 'black',\n",
       " 'body',\n",
       " 'book',\n",
       " 'born',\n",
       " 'boy',\n",
       " 'boyfriend',\n",
       " 'break',\n",
       " 'breaking',\n",
       " 'bring',\n",
       " 'broke',\n",
       " 'brother',\n",
       " 'brought',\n",
       " 'business',\n",
       " 'ca',\n",
       " 'call',\n",
       " 'called',\n",
       " 'calling',\n",
       " 'came',\n",
       " 'campaign',\n",
       " 'canâ\\x80\\x99t',\n",
       " 'car',\n",
       " 'care',\n",
       " 'career',\n",
       " 'case',\n",
       " 'cast',\n",
       " 'celebrity',\n",
       " 'ceremony',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'character',\n",
       " 'characters',\n",
       " 'child',\n",
       " 'children',\n",
       " 'claimed',\n",
       " 'claims',\n",
       " 'clear',\n",
       " 'close',\n",
       " 'come',\n",
       " 'comedy',\n",
       " 'comes',\n",
       " 'coming',\n",
       " 'comment',\n",
       " 'comments',\n",
       " 'company',\n",
       " 'completely',\n",
       " 'concert',\n",
       " 'confirmed',\n",
       " 'continue',\n",
       " 'continued',\n",
       " 'could',\n",
       " 'country',\n",
       " 'couple',\n",
       " 'course',\n",
       " 'court',\n",
       " 'cover',\n",
       " 'currently',\n",
       " 'cut',\n",
       " 'date',\n",
       " 'dating',\n",
       " 'daughter',\n",
       " 'day',\n",
       " 'days',\n",
       " 'de',\n",
       " 'deal',\n",
       " 'death',\n",
       " 'debut',\n",
       " 'decided',\n",
       " 'decision',\n",
       " 'definitely',\n",
       " 'delivered',\n",
       " 'described',\n",
       " 'despite',\n",
       " 'didnâ\\x80\\x99t',\n",
       " 'died',\n",
       " 'different',\n",
       " 'dinner',\n",
       " 'director',\n",
       " 'divorce',\n",
       " 'doesnâ\\x80\\x99t',\n",
       " 'done',\n",
       " 'donâ\\x80\\x99t',\n",
       " 'drama',\n",
       " 'dress',\n",
       " 'due',\n",
       " 'earlier',\n",
       " 'early',\n",
       " 'edit',\n",
       " 'either',\n",
       " 'else',\n",
       " 'end',\n",
       " 'ended',\n",
       " 'engaged',\n",
       " 'engagement',\n",
       " 'enough',\n",
       " 'entire',\n",
       " 'episode',\n",
       " 'episodes',\n",
       " 'especially',\n",
       " 'even',\n",
       " 'event',\n",
       " 'events',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'excited',\n",
       " 'executive',\n",
       " 'experience',\n",
       " 'explained',\n",
       " 'face',\n",
       " 'fact',\n",
       " 'families',\n",
       " 'family',\n",
       " 'famous',\n",
       " 'fan',\n",
       " 'fans',\n",
       " 'far',\n",
       " 'fashion',\n",
       " 'father',\n",
       " 'favorite',\n",
       " 'featured',\n",
       " 'features',\n",
       " 'feel',\n",
       " 'feeling',\n",
       " 'feels',\n",
       " 'felt',\n",
       " 'female',\n",
       " 'fight',\n",
       " 'filed',\n",
       " 'film',\n",
       " 'filming',\n",
       " 'films',\n",
       " 'final',\n",
       " 'finally',\n",
       " 'find',\n",
       " 'first',\n",
       " 'five',\n",
       " 'followed',\n",
       " 'following',\n",
       " 'former',\n",
       " 'forward',\n",
       " 'found',\n",
       " 'four',\n",
       " 'friend',\n",
       " 'friends',\n",
       " 'front',\n",
       " 'full',\n",
       " 'fun',\n",
       " 'future',\n",
       " 'game',\n",
       " 'gave',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'girl',\n",
       " 'girlfriend',\n",
       " 'girls',\n",
       " 'give',\n",
       " 'given',\n",
       " 'giving',\n",
       " 'go',\n",
       " 'goes',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'good',\n",
       " 'got',\n",
       " 'government',\n",
       " 'great',\n",
       " 'group',\n",
       " 'guest',\n",
       " 'guy',\n",
       " 'hair',\n",
       " 'half',\n",
       " 'hand',\n",
       " 'hands',\n",
       " 'happen',\n",
       " 'happened',\n",
       " 'happy',\n",
       " 'harassment',\n",
       " 'hard',\n",
       " 'head',\n",
       " 'health',\n",
       " 'hear',\n",
       " 'heard',\n",
       " 'heart',\n",
       " 'held',\n",
       " 'help',\n",
       " 'heâ\\x80\\x99s',\n",
       " 'high',\n",
       " 'history',\n",
       " 'hit',\n",
       " 'home',\n",
       " 'hope',\n",
       " 'host',\n",
       " 'hot',\n",
       " 'hours',\n",
       " 'house',\n",
       " 'however',\n",
       " 'https',\n",
       " 'huge',\n",
       " 'husband',\n",
       " 'idea',\n",
       " 'image',\n",
       " 'immediately',\n",
       " 'important',\n",
       " 'include',\n",
       " 'included',\n",
       " 'including',\n",
       " 'industry',\n",
       " 'information',\n",
       " 'insider',\n",
       " 'instead',\n",
       " 'interview',\n",
       " 'involved',\n",
       " 'isnâ\\x80\\x99t',\n",
       " 'issue',\n",
       " 'issues',\n",
       " 'itâ\\x80\\x99s',\n",
       " 'job',\n",
       " 'joined',\n",
       " 'keep',\n",
       " 'kept',\n",
       " 'kids',\n",
       " 'kind',\n",
       " 'knew',\n",
       " 'know',\n",
       " 'known',\n",
       " 'knows',\n",
       " 'last',\n",
       " 'late',\n",
       " 'later',\n",
       " 'latest',\n",
       " 'law',\n",
       " 'lead',\n",
       " 'least',\n",
       " 'leave',\n",
       " 'leaving',\n",
       " 'led',\n",
       " 'left',\n",
       " 'legal',\n",
       " 'less',\n",
       " 'let',\n",
       " 'life',\n",
       " 'like',\n",
       " 'likely',\n",
       " 'line',\n",
       " 'list',\n",
       " 'little',\n",
       " 'live',\n",
       " 'lives',\n",
       " 'living',\n",
       " 'long',\n",
       " 'longer',\n",
       " 'look',\n",
       " 'looked',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'lost',\n",
       " 'lot',\n",
       " 'love',\n",
       " 'loved',\n",
       " 'made',\n",
       " 'magazine',\n",
       " 'major',\n",
       " 'make',\n",
       " 'makes',\n",
       " 'making',\n",
       " 'man',\n",
       " 'many',\n",
       " 'marriage',\n",
       " 'married',\n",
       " 'matter',\n",
       " 'may',\n",
       " 'maybe',\n",
       " 'mean',\n",
       " 'means',\n",
       " 'media',\n",
       " 'meet',\n",
       " 'meeting',\n",
       " 'member',\n",
       " 'members',\n",
       " 'men',\n",
       " 'met',\n",
       " 'might',\n",
       " 'million',\n",
       " 'model',\n",
       " 'mom',\n",
       " 'moment',\n",
       " 'money',\n",
       " 'month',\n",
       " 'months',\n",
       " 'morning',\n",
       " 'mother',\n",
       " 'move',\n",
       " 'moved',\n",
       " 'movie',\n",
       " 'much',\n",
       " 'multiple',\n",
       " 'music',\n",
       " 'must',\n",
       " \"n't\",\n",
       " 'name',\n",
       " 'named',\n",
       " 'national',\n",
       " 'nearly',\n",
       " 'need',\n",
       " 'needed',\n",
       " 'network',\n",
       " 'never',\n",
       " 'new',\n",
       " 'news',\n",
       " 'next',\n",
       " 'night',\n",
       " 'noted',\n",
       " 'nothing',\n",
       " 'number',\n",
       " 'office',\n",
       " 'official',\n",
       " 'often',\n",
       " 'old',\n",
       " 'one',\n",
       " 'open',\n",
       " 'opened',\n",
       " 'opening',\n",
       " 'order',\n",
       " 'original',\n",
       " 'others',\n",
       " 'outside',\n",
       " 'paid',\n",
       " 'pair',\n",
       " 'parents',\n",
       " 'part',\n",
       " 'party',\n",
       " 'past',\n",
       " 'pay',\n",
       " 'people',\n",
       " 'percent',\n",
       " 'perfect',\n",
       " 'performance',\n",
       " 'performed',\n",
       " 'person',\n",
       " 'personal',\n",
       " 'phone',\n",
       " 'photo',\n",
       " 'photos',\n",
       " 'picture',\n",
       " 'place',\n",
       " 'plan',\n",
       " 'plans',\n",
       " 'play',\n",
       " 'played',\n",
       " 'playing',\n",
       " 'point',\n",
       " 'police',\n",
       " 'political',\n",
       " 'pop',\n",
       " 'popular',\n",
       " 'possible',\n",
       " 'post',\n",
       " 'posted',\n",
       " 'power',\n",
       " 'pregnancy',\n",
       " 'pregnant',\n",
       " 'premiere',\n",
       " 'president',\n",
       " 'press',\n",
       " 'pretty',\n",
       " 'previous',\n",
       " 'previously',\n",
       " 'private',\n",
       " 'probably',\n",
       " 'process',\n",
       " 'producer',\n",
       " 'production',\n",
       " 'public',\n",
       " 'published',\n",
       " 'push',\n",
       " 'put',\n",
       " 'question',\n",
       " 'quite',\n",
       " 'rapper',\n",
       " 'reached',\n",
       " 'read',\n",
       " 'ready',\n",
       " 'real',\n",
       " 'reality',\n",
       " 'really',\n",
       " 'reason',\n",
       " 'received',\n",
       " 'recent',\n",
       " 'recently',\n",
       " 'record',\n",
       " 'red',\n",
       " 'relationship',\n",
       " 'release',\n",
       " 'released',\n",
       " 'report',\n",
       " 'reported',\n",
       " 'reportedly',\n",
       " 'reports',\n",
       " 'response',\n",
       " 'rest',\n",
       " 'return',\n",
       " 'returned',\n",
       " 'revealed',\n",
       " 'right',\n",
       " 'rights',\n",
       " 'ring',\n",
       " 'role',\n",
       " 'roles',\n",
       " 'romance',\n",
       " 'romantic',\n",
       " 'room',\n",
       " 'royal',\n",
       " 'rumors',\n",
       " 'run',\n",
       " 'said',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'scene',\n",
       " 'school',\n",
       " 'season',\n",
       " 'second',\n",
       " 'see',\n",
       " 'seeing',\n",
       " 'seemed',\n",
       " 'seems',\n",
       " 'seen',\n",
       " 'sense',\n",
       " 'series',\n",
       " 'service',\n",
       " 'set',\n",
       " 'seven',\n",
       " 'several',\n",
       " 'sex',\n",
       " 'sexual',\n",
       " 'share',\n",
       " 'shared',\n",
       " 'sheâ\\x80\\x99s',\n",
       " 'shot',\n",
       " 'show',\n",
       " 'showed',\n",
       " 'shows',\n",
       " 'side',\n",
       " 'signed',\n",
       " 'since',\n",
       " 'singer',\n",
       " 'single',\n",
       " 'sister',\n",
       " 'six',\n",
       " 'small',\n",
       " 'social',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'son',\n",
       " 'song',\n",
       " 'songs',\n",
       " 'soon',\n",
       " 'source',\n",
       " 'speak',\n",
       " 'special',\n",
       " 'speech',\n",
       " 'spending',\n",
       " 'spent',\n",
       " 'split',\n",
       " 'spoke',\n",
       " 'spotted',\n",
       " 'stage',\n",
       " 'star',\n",
       " 'starred',\n",
       " 'stars',\n",
       " 'start',\n",
       " 'started',\n",
       " 'state',\n",
       " 'stated',\n",
       " 'statement',\n",
       " 'stay',\n",
       " 'still',\n",
       " 'stop',\n",
       " 'stories',\n",
       " 'story',\n",
       " 'straight',\n",
       " 'strong',\n",
       " 'studio',\n",
       " 'style',\n",
       " 'success',\n",
       " 'summer',\n",
       " 'support',\n",
       " 'sure',\n",
       " 'take',\n",
       " 'taken',\n",
       " 'takes',\n",
       " 'taking',\n",
       " 'talk',\n",
       " 'talking',\n",
       " 'tax',\n",
       " 'team',\n",
       " 'television',\n",
       " 'tell',\n",
       " 'telling',\n",
       " 'tells',\n",
       " 'thatâ\\x80\\x99s',\n",
       " 'thereâ\\x80\\x99s',\n",
       " 'theyâ\\x80\\x99re',\n",
       " 'thing',\n",
       " 'things',\n",
       " 'think',\n",
       " 'third',\n",
       " 'though',\n",
       " 'thought',\n",
       " 'three',\n",
       " 'throughout',\n",
       " 'time',\n",
       " 'times',\n",
       " 'title',\n",
       " 'today',\n",
       " 'together',\n",
       " 'told',\n",
       " 'took',\n",
       " 'top',\n",
       " 'tour',\n",
       " 'track',\n",
       " 'tried',\n",
       " 'true',\n",
       " 'truth',\n",
       " 'try',\n",
       " 'trying',\n",
       " 'turn',\n",
       " 'turned',\n",
       " 'tweeted',\n",
       " 'two',\n",
       " 'understand',\n",
       " 'upcoming',\n",
       " 'us',\n",
       " 'use',\n",
       " 'used',\n",
       " 'using',\n",
       " 'version',\n",
       " 'via',\n",
       " 'video',\n",
       " 'viewers',\n",
       " 'visit',\n",
       " 'want',\n",
       " 'wanted',\n",
       " 'wants',\n",
       " 'war',\n",
       " 'wasnâ\\x80\\x99t',\n",
       " 'watch',\n",
       " 'watching',\n",
       " 'way',\n",
       " 'ways',\n",
       " 'wearing',\n",
       " 'website',\n",
       " 'wedding',\n",
       " 'week',\n",
       " 'weekend',\n",
       " 'weeks',\n",
       " 'well',\n",
       " 'went',\n",
       " 'weâ\\x80\\x99re',\n",
       " 'whether',\n",
       " 'white',\n",
       " 'whole',\n",
       " 'whose',\n",
       " 'wife',\n",
       " 'win',\n",
       " 'within',\n",
       " 'without',\n",
       " 'woman',\n",
       " 'women',\n",
       " 'words',\n",
       " 'wore',\n",
       " 'work',\n",
       " 'worked',\n",
       " 'working',\n",
       " 'world',\n",
       " 'worth',\n",
       " 'would',\n",
       " 'writing',\n",
       " 'written',\n",
       " 'wrong',\n",
       " 'wrote',\n",
       " 'year',\n",
       " 'years',\n",
       " 'yet',\n",
       " 'young',\n",
       " 'youâ\\x80\\x99re',\n",
       " '»',\n",
       " 'â\\x80\\x93',\n",
       " 'â\\x80\\x94',\n",
       " 'â\\x80\\x99',\n",
       " 'â\\x80\\x9cI',\n",
       " 'â\\x80\\x9cIt',\n",
       " 'â\\x80\\x9cItâ\\x80\\x99s',\n",
       " 'â\\x80\\x9cThe',\n",
       " 'â\\x80\\x9cWe',\n",
       " 'â\\x80\\x9d',\n",
       " 'â\\x80¦']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(most_words)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### most_words觀察到:\n",
    "需要的前處理為 做詞性還原(Aw,Awww)、去除雜字雜符號、數字($&...)、將emotion用空格替代(XD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "875d77c51bc67d1fdc257e2951ef9722f2c3c3f9"
   },
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "2b8bf4694b417cafb10c592cde1c044390e43be8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "#取字根\n",
    "def stem_tokenize(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmer = WordNetLemmatizer()\n",
    "    return [stemmer.lemmatize(token) for token in word_tokenize(text)]  \n",
    "#還原詞性\n",
    "def lemmatize_tokenize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2c7ab7592afcdc9199eb2aa95b4d4fb819527d5b"
   },
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "e8ec37f0fd4a0035440da78495500c0d8ecef5c1"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6ac3748efa62b760d9c6d814d8bf727a75165e89"
   },
   "source": [
    "#### Building the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "6d02e45a46b632277e17c3e6de2de1ecda255ed3"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "ae1998949f35a5a8f8e9c3e2fc8fd32441a2cb73"
   },
   "outputs": [],
   "source": [
    "class TextPreProc(BaseEstimator,TransformerMixin):       \n",
    "    def __init__(self, use_mention=False):\n",
    "        self.use_mention = use_mention\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # We can choose between keeping the mentions\n",
    "        # or deleting them\n",
    "        if self.use_mention:\n",
    "            X = X.str.replace(r\"@[a-zA-Z0-9_]* \", \" @tags \")  \n",
    "        else:\n",
    "            X = X.str.replace(r\"@[a-zA-Z0-9_]* \", \"\")\n",
    "            \n",
    "        # Keeping only the word after the #\n",
    "        X = X.str.replace(\"#\", \"\")\n",
    "        X = X.str.replace(r\"[-\\.\\n]\", \"\")\n",
    "        \n",
    "        # !#$&\n",
    "        X = X.str.replace(r\"[!#$&]\", \"\")\n",
    "        # ' or 'xxx\n",
    "        X = X.str.replace(r\"'[\\w'\\s]*\", \"\")\n",
    "        # ) ( ...\n",
    "        X = X.str.replace(r\"'[\\D]\", \"\")\n",
    "        # number\n",
    "        X = X.str.replace(r\"[\\d]*\", \"\")\n",
    "        \n",
    "        # Removing HTML garbage\n",
    "        X = X.str.replace(r\"&\\w+;\", \"\")\n",
    "        # Removing links\n",
    "        X = X.str.replace(r\"https?://\\S*\", \"\")\n",
    "        # replace repeated letters with only two occurences\n",
    "        # heeeelllloooo => heelloo\n",
    "        X = X.str.replace(r\"(.)\\1+\", r\"\\1\\1\")\n",
    "        # mark emoticons as happy or sad\n",
    "        X = X.str.replace(a, \"\")\n",
    "        X = X.str.lower()   \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "99f19e43bfeedd6c51bb96e3bdbcc7d901088085"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "label = train_data['label']\n",
    "text = train_data['text']\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=lemmatize_tokenize, ngram_range=(1,2),max_features=5000) \n",
    "pipeline = Pipeline([\n",
    "    ('text_pre_processing', TextPreProc(use_mention=False)),  \n",
    "    ('vectorizer', vectorizer),                               \n",
    "])  \n",
    "\n",
    "\n",
    "learn_data, test_data, label_learning, label_test = train_test_split(text, label, test_size=0.3)\n",
    "#x_train,x_test,y_train,y_test\n",
    "\n",
    "\n",
    "learning_data = pipeline.fit_transform(learn_data)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "81d61a981d1c32ddbe6932f0b968b9ab224538af"
   },
   "source": [
    "# Select a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "0c4d82dc1ffb4872c8bf605a9f6212c7f785dc65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== logitic regression ===\n",
      "scores =  [0.63865546 0.7219917  0.69672131 0.57798165 0.66956522 0.66666667\n",
      " 0.69076305 0.63793103 0.63755459 0.62393162]\n",
      "mean =  0.6561762306115272\n",
      "variance =  0.0015601879270126344\n",
      "score on the learning data (accuracy) =  0.8426934097421204\n",
      "\n",
      "=== bernoulliNB ===\n",
      "scores =  [0.536      0.60557769 0.65546218 0.59607843 0.62948207 0.63779528\n",
      " 0.64367816 0.66129032 0.63934426 0.62745098]\n",
      "mean =  0.6232159378980648\n",
      "variance =  0.0012092233636768158\n",
      "score on the learning data (accuracy) =  0.767621776504298\n",
      "\n",
      "=== multinomialNB ===\n",
      "scores =  [0.62831858 0.72727273 0.62337662 0.52132701 0.61111111 0.63063063\n",
      " 0.64935065 0.60444444 0.64035088 0.59821429]\n",
      "mean =  0.6234396947382261\n",
      "variance =  0.002339031569580384\n",
      "score on the learning data (accuracy) =  0.805730659025788\n",
      "\n",
      "=== lightGBM ===\n",
      "scores =  [0.71713147 0.74909091 0.68656716 0.6097561  0.688      0.7007874\n",
      " 0.72992701 0.704      0.65338645 0.664     ]\n",
      "mean =  0.6902646507991915\n",
      "variance =  0.0014605105946071447\n",
      "score on the learning data (accuracy) =  0.9931232091690544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "\n",
    "import lightgbm\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "\n",
    "lr = LogisticRegression()\n",
    "bnb = BernoulliNB()\n",
    "mnb = MultinomialNB()\n",
    "lgbm = LGBMClassifier()\n",
    "\n",
    "models = {\n",
    "    'logitic regression': lr,\n",
    "    'bernoulliNB': bnb,\n",
    "    'multinomialNB': mnb,           \n",
    "    'lightGBM' : lgbm,\n",
    "}\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "f1_scorer = make_scorer(f1_score, pos_label=\"1\")\n",
    "for model in models.keys():\n",
    "    scores = cross_val_score(models[model], learning_data, label_learning, scoring=f1_scorer, cv=10)\n",
    "    print(\"===\", model, \"===\")\n",
    "    print(\"scores = \", scores)\n",
    "    print(\"mean = \", scores.mean())\n",
    "    print(\"variance = \", scores.var())\n",
    "    models[model].fit(learning_data, label_learning)   \n",
    "    print(\"score on the learning data (accuracy) = \", accuracy_score(models[model].predict(learning_data), label_learning))\n",
    "    print(\"\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "569dfd9fd7e6244e1b20fc5bacf877799b2ddc85"
   },
   "source": [
    "# Fine tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# grid_search_pipeline = Pipeline([\n",
    "#     ('text_pre_processing', TextPreProc()),\n",
    "#     ('vectorizer', TfidfVectorizer()),\n",
    "#     ('model', LGBMClassifier()),      #想調這三個東西裡面的參數  (比較完三個model後，我們最後只想用multinomialNB當預測model)\n",
    "# ])\n",
    "\n",
    "# params = [\n",
    "#     {\n",
    "#         'text_pre_processing__use_mention': [True, False],\n",
    "#         'vectorizer__max_features': [1000, 2000, 5000, 10000, 20000, None],\n",
    "#         'vectorizer__ngram_range': [(1,1), (1,2)],      #這三個東西裡面分別參數的可能值\n",
    "#     },\n",
    "# ]\n",
    "# from sklearn.metrics import f1_score\n",
    "# from sklearn.metrics import make_scorer\n",
    "# f1_scorer = make_scorer(f1_score, pos_label=\"1\")\n",
    "# grid_search = GridSearchCV(grid_search_pipeline, params, cv=5, scoring=f1_scorer)\n",
    "# grid_search.fit(learn_data, label_learning )   #用x_train,y_train去調\n",
    "# print(grid_search.best_params_)    #得到這些結果 再拉上去改這些參數的值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "16c0270e6a97b641762eb9291bcf7d5ad491f450"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "30627c10aeb7c4cee488a431a7871a6e80c51baa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
       "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm\n",
    "from lightgbm import LGBMClassifier\n",
    "lgbm = LGBMClassifier()\n",
    "lgbm.fit(learning_data, label_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "1778d039bff617c9c38dd6cf73d7e814d819ab7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7747326203208557"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data = pipeline.transform(test_data)   #tfidf轉向量:fit_transform()=fit()+transform()\n",
    "lgbm.score(testing_data, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "de2fd39a5c5c67e96ba8860d7f272795021d7253"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id label\n",
      "0        2     0\n",
      "1        3     0\n",
      "2        4     1\n",
      "3        5     1\n",
      "4        6     0\n",
      "...    ...   ...\n",
      "1242  1244     0\n",
      "1243  1245     0\n",
      "1244  1246     1\n",
      "1245  1247     0\n",
      "1246  1248     0\n",
      "\n",
      "[1247 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Predecting on the test.csv\n",
    "sub_data = pd.read_csv(\"test.csv\", encoding='ISO-8859-1',delimiter=\"\\t\")\n",
    "sub_learning = pipeline.transform(sub_data.text)        #test.csv沒告訴你真正label所以不能measure accuracy，且test.csv只能做將文字轉向量\n",
    "sub = pd.DataFrame(sub_data.id, columns=(\"id\", \"label\"))   \n",
    "sub[\"label\"] = lgbm.predict(sub_learning)\n",
    "print(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
